---
phase: 02-document-processing
plan: 05
type: execute
wave: 2
depends_on: ["02-01", "02-02", "02-03", "02-04"]
files_modified:
  - internal/indexer/pipeline.go
  - internal/indexer/pipeline_test.go
autonomous: true
user_setup:
  - service: openai
    why: "Embeddings and metadata generation"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Platform -> API keys (https://platform.openai.com/api-keys)"
    dashboard_config: []
  - service: github
    why: "GitHub API access for fetching documentation (optional but recommended)"
    env_vars:
      - name: GITHUB_TOKEN
        source: "GitHub Settings -> Developer settings -> Personal access tokens -> Tokens (classic)"
    dashboard_config: []

must_haves:
  truths:
    - "Pipeline fetches all EINO docs from GitHub"
    - "Pipeline chunks documents at header boundaries"
    - "Pipeline generates embeddings for all chunks"
    - "Pipeline generates metadata (summary, entities) for each document"
    - "Pipeline stores documents and chunks in Qdrant with current commit SHA"
  artifacts:
    - path: "internal/indexer/pipeline.go"
      provides: "End-to-end indexing pipeline"
      exports: ["Pipeline", "NewPipeline", "IndexAll", "IndexResult"]
    - path: "internal/indexer/pipeline_test.go"
      provides: "Integration test for full pipeline"
  key_links:
    - from: "internal/indexer/pipeline.go"
      to: "internal/github/fetcher.go"
      via: "Document fetching"
      pattern: "fetcher\\.ListDocs|fetcher\\.FetchDoc"
    - from: "internal/indexer/pipeline.go"
      to: "internal/markdown/chunker.go"
      via: "Document chunking"
      pattern: "chunker\\.ChunkDocument"
    - from: "internal/indexer/pipeline.go"
      to: "internal/embedding/embedder.go"
      via: "Embedding generation"
      pattern: "embedder\\.GenerateEmbeddings"
    - from: "internal/indexer/pipeline.go"
      to: "internal/metadata/generator.go"
      via: "Metadata generation"
      pattern: "generator\\.GenerateMetadata"
    - from: "internal/indexer/pipeline.go"
      to: "internal/storage/qdrant.go"
      via: "Storage operations"
      pattern: "storage\\.UpsertDocument|storage\\.UpsertChunks"
---

<objective>
Implement end-to-end indexing pipeline

Purpose: Orchestrate all components (fetcher, chunker, embedder, metadata generator, storage) into a complete indexing pipeline that processes EINO documentation from GitHub into Qdrant.

Output: Pipeline that indexes all EINO docs with a single call, ready for Phase 4's sync trigger.
</objective>

<execution_context>
@/home/bull/.claude/get-shit-done/workflows/execute-plan.md
@/home/bull/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-processing/02-RESEARCH.md
@.planning/phases/02-document-processing/02-CONTEXT.md

# Prior plan summaries (for component interfaces)
@.planning/phases/02-document-processing/02-01-SUMMARY.md
@.planning/phases/02-document-processing/02-02-SUMMARY.md
@.planning/phases/02-document-processing/02-03-SUMMARY.md
@.planning/phases/02-document-processing/02-04-SUMMARY.md

# Storage interface
@internal/storage/models.go
@internal/storage/qdrant.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement indexing pipeline</name>
  <files>internal/indexer/pipeline.go</files>
  <action>
Create orchestrating pipeline that coordinates all components:

1. Define IndexResult struct:
   ```go
   type IndexResult struct {
       TotalDocs      int
       TotalChunks    int
       SuccessfulDocs int
       FailedDocs     []FailedDoc
       CommitSHA      string
       Duration       time.Duration
   }

   type FailedDoc struct {
       Path   string
       Reason string
   }
   ```

2. Define Pipeline struct:
   ```go
   type Pipeline struct {
       fetcher   *github.Fetcher
       chunker   *markdown.Chunker
       embedder  *embedding.Embedder
       generator *metadata.Generator
       storage   *storage.QdrantStorage
       logger    *slog.Logger
   }
   ```

3. NewPipeline function:
   - Takes all component instances
   - Returns configured Pipeline

4. IndexAll method - main orchestration:
   ```go
   func (p *Pipeline) IndexAll(ctx context.Context) (*IndexResult, error) {
       start := time.Now()
       result := &IndexResult{}

       // 1. Get latest commit SHA
       commitSHA, err := p.fetcher.GetLatestCommitSHA(ctx)
       if err != nil {
           return nil, fmt.Errorf("get commit SHA: %w", err)
       }
       result.CommitSHA = commitSHA
       p.logger.Info("Starting indexing", "commit", commitSHA)

       // 2. List all docs
       paths, err := p.fetcher.ListDocs(ctx)
       if err != nil {
           return nil, fmt.Errorf("list docs: %w", err)
       }
       result.TotalDocs = len(paths)
       p.logger.Info("Found documents", "count", len(paths))

       // 3. Process each document
       for _, path := range paths {
           err := p.processDocument(ctx, path, commitSHA)
           if err != nil {
               p.logger.Warn("Failed to process document", "path", path, "error", err)
               result.FailedDocs = append(result.FailedDocs, FailedDoc{
                   Path:   path,
                   Reason: err.Error(),
               })
               continue  // Skip unparseable docs, continue with others
           }
           result.SuccessfulDocs++
       }

       result.Duration = time.Since(start)
       p.logger.Info("Indexing complete",
           "successful", result.SuccessfulDocs,
           "failed", len(result.FailedDocs),
           "duration", result.Duration,
       )

       return result, nil
   }
   ```

5. processDocument helper - single document processing:
   ```go
   func (p *Pipeline) processDocument(ctx context.Context, path, commitSHA string) error {
       // Fetch content
       fetched, err := p.fetcher.FetchDoc(ctx, path)
       if err != nil {
           return fmt.Errorf("fetch: %w", err)
       }
       p.logger.Debug("Fetched document", "path", path, "size", len(fetched.Content))

       // Generate metadata (summary, entities)
       meta, err := p.generator.GenerateMetadata(ctx, path, fetched.Content)
       if err != nil {
           p.logger.Warn("Metadata generation failed, using empty", "path", path, "error", err)
           meta = &metadata.DocumentMetadata{Summary: "", Entities: []string{}}
       }

       // Chunk document
       chunks := p.chunker.ChunkDocument(fetched.Content)
       p.logger.Debug("Chunked document", "path", path, "chunks", len(chunks))

       // Generate embeddings for all chunks
       texts := make([]string, len(chunks))
       for i, chunk := range chunks {
           texts[i] = chunk.Content  // Content already has header path prepended
       }

       embeddings, err := p.embedder.GenerateEmbeddings(ctx, texts)
       if err != nil {
           return fmt.Errorf("embeddings: %w", err)
       }

       // Create parent document
       docID := uuid.New().String()
       doc := &storage.Document{
           ID:      docID,
           Content: fetched.Content,
           Metadata: storage.DocumentMetadata{
               Path:       path,
               URL:        fetched.URL,
               Repository: "cloudwego/cloudwego.github.io",
               CommitSHA:  commitSHA,
               IndexedAt:  time.Now(),
               Summary:    meta.Summary,
               Entities:   meta.Entities,
           },
       }

       if err := p.storage.UpsertDocument(ctx, doc); err != nil {
           return fmt.Errorf("store document: %w", err)
       }

       // Create chunks with embeddings
       storageChunks := make([]*storage.Chunk, len(chunks))
       for i, chunk := range chunks {
           storageChunks[i] = &storage.Chunk{
               ID:          uuid.New().String(),
               ParentDocID: docID,
               ChunkIndex:  chunk.Index,
               HeaderPath:  chunk.HeaderPath,
               Content:     chunk.RawContent,  // Store without header prefix in payload
               Path:        path,
               Repository:  "cloudwego/cloudwego.github.io",
               Embedding:   embeddings[i],
           }
       }

       if err := p.storage.UpsertChunks(ctx, storageChunks); err != nil {
           return fmt.Errorf("store chunks: %w", err)
       }

       p.logger.Info("Indexed document", "path", path, "chunks", len(chunks))
       return nil
   }
   ```

6. Key behaviors (from CONTEXT.md):
   - Skip unparseable markdown with warning, continue others
   - Metadata failure: use empty values, don't fail document
   - Embedding failure: fail document (required for search)
   - Log each file processed with timing

7. Decision (from CONTEXT.md): Deleted documents - don't handle in this phase. Manual sync trigger in Phase 4 will clear and re-index.
  </action>
  <verify>
go build ./internal/indexer/...
  </verify>
  <done>
Pipeline orchestrates fetch->chunk->embed->store flow; compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Create integration test</name>
  <files>internal/indexer/pipeline_test.go</files>
  <action>
Create integration test that validates full pipeline:

1. TestPipeline_IndexAll_Integration:
   - Skip if OPENAI_API_KEY not set (use testing.Short or build tag)
   - Create all real components
   - Run IndexAll on actual EINO docs
   - Verify:
     - Result.TotalDocs > 0
     - Result.SuccessfulDocs > 0
     - Documents are searchable in Qdrant after indexing

2. Test structure:
   ```go
   func TestPipeline_IndexAll_Integration(t *testing.T) {
       if os.Getenv("OPENAI_API_KEY") == "" {
           t.Skip("OPENAI_API_KEY not set, skipping integration test")
       }

       // Setup
       storage, err := storage.NewQdrantStorage("localhost", 6334)
       require.NoError(t, err)
       defer storage.Close()

       // Clear existing data for clean test
       err = storage.ClearCollection(context.Background())
       require.NoError(t, err)
       err = storage.EnsureCollection(context.Background())
       require.NoError(t, err)

       // Create components
       ghClient := github.NewClient()
       fetcher := github.NewFetcher(ghClient, "cloudwego", "cloudwego.github.io", "content/en/docs/eino")
       chunker := markdown.NewChunker()

       openaiClient, err := embedding.NewClient()
       require.NoError(t, err)
       embedder := embedding.NewEmbedder(openaiClient)
       generator := metadata.NewGenerator(openaiClient.Client())

       pipeline := NewPipeline(fetcher, chunker, embedder, generator, storage, slog.Default())

       // Run indexing (limit to first 3 docs for speed)
       // Note: Full indexing tested manually, this validates wiring
       ctx := context.Background()
       result, err := pipeline.IndexAll(ctx)
       require.NoError(t, err)

       // Verify results
       assert.Greater(t, result.TotalDocs, 0)
       assert.Greater(t, result.SuccessfulDocs, 0)
       assert.NotEmpty(t, result.CommitSHA)

       // Verify searchable
       testQuery := make([]float32, 1536)  // Zero vector for simple test
       chunks, err := storage.SearchChunks(ctx, testQuery, 5, "cloudwego/cloudwego.github.io")
       require.NoError(t, err)
       assert.Greater(t, len(chunks), 0, "Should find indexed chunks")
   }
   ```

3. Add build tag for integration tests:
   ```go
   //go:build integration

   package indexer
   ```

This allows running `go test -tags=integration ./...` for full tests.
  </action>
  <verify>
go build ./internal/indexer/...
# Integration test requires OPENAI_API_KEY and running Qdrant
# go test -tags=integration -v ./internal/indexer/...
  </verify>
  <done>
Integration test validates full pipeline flow; compiles without errors
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `go build ./...` succeeds
2. Pipeline wires all components together
3. Integration test validates end-to-end flow
4. Result struct provides detailed indexing statistics
</verification>

<success_criteria>
- Pipeline fetches docs from GitHub
- Pipeline chunks documents at header boundaries
- Pipeline generates embeddings for all chunks
- Pipeline generates metadata for each document
- Pipeline stores everything in Qdrant with commit SHA
- Detailed logging tracks progress
- Integration test proves full flow works
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-processing/02-05-SUMMARY.md`
</output>
