---
phase: 02-document-processing
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/embedding/client.go
  - internal/embedding/embedder.go
autonomous: true
user_setup:
  - service: openai
    why: "Embedding generation using text-embedding-3-small"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Platform -> API keys (https://platform.openai.com/api-keys)"
    dashboard_config: []

must_haves:
  truths:
    - "Embedder generates 1536-dimensional vectors using text-embedding-3-small"
    - "Embedder batches requests for efficiency (up to 500 texts per batch)"
    - "Embedder implements exponential backoff on rate limit errors"
  artifacts:
    - path: "internal/embedding/client.go"
      provides: "OpenAI client wrapper"
      exports: ["NewClient", "Client"]
    - path: "internal/embedding/embedder.go"
      provides: "Embedding generation with batching"
      exports: ["Embedder", "NewEmbedder", "GenerateEmbeddings"]
  key_links:
    - from: "internal/embedding/embedder.go"
      to: "openai/openai-go"
      via: "API calls"
      pattern: "client\\.Embeddings\\.New"
    - from: "internal/embedding/embedder.go"
      to: "cenkalti/backoff"
      via: "Retry logic"
      pattern: "backoff\\.Retry"
---

<objective>
Implement OpenAI embeddings client with batching and retry

Purpose: Generate embeddings for document chunks using OpenAI's text-embedding-3-small model. Batching maximizes throughput, and exponential backoff handles rate limits gracefully.

Output: Embedder that converts text to 1536-dimensional vectors with proper error handling.
</objective>

<execution_context>
@/home/bull/.claude/get-shit-done/workflows/execute-plan.md
@/home/bull/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-processing/02-RESEARCH.md
@internal/storage/models.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OpenAI client wrapper</name>
  <files>internal/embedding/client.go</files>
  <action>
Create OpenAI client wrapper:

1. Add dependency:
   - go get github.com/openai/openai-go

2. Define Client struct:
   - Wraps *openai.Client
   - Stores API key (from OPENAI_API_KEY env var)

3. NewClient function:
   - Reads OPENAI_API_KEY from environment
   - Returns error if key not set
   - Creates openai.Client with key
   - Returns Client struct

Key pattern:
```go
import "github.com/openai/openai-go"

func NewClient() (*Client, error) {
    apiKey := os.Getenv("OPENAI_API_KEY")
    if apiKey == "" {
        return nil, fmt.Errorf("OPENAI_API_KEY environment variable not set")
    }

    client := openai.NewClient()  // Uses OPENAI_API_KEY automatically
    return &Client{client: client}, nil
}
```

Note: openai-go reads OPENAI_API_KEY automatically, but we validate it exists first for better error messages.
  </action>
  <verify>
go build ./internal/embedding/...
  </verify>
  <done>
Client wrapper compiles and validates API key presence
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement embedder with batching and retry</name>
  <files>internal/embedding/embedder.go</files>
  <action>
Create Embedder with batching and exponential backoff:

1. Define Embedder struct:
   - Embeds Client
   - Stores batch size (default 500)

2. NewEmbedder function:
   - Takes Client, optional batch size
   - Returns configured Embedder

3. GenerateEmbeddings method:
   - Input: []string texts
   - Output: [][]float32 embeddings (matches storage.Chunk.Embedding type)

   Implementation:
   ```go
   func (e *Embedder) GenerateEmbeddings(ctx context.Context, texts []string) ([][]float32, error) {
       var allEmbeddings [][]float32
       batchSize := 500  // Balance RPM vs TPM

       for i := 0; i < len(texts); i += batchSize {
           end := min(i+batchSize, len(texts))
           batch := texts[i:end]

           embeddings, err := e.embedBatchWithRetry(ctx, batch)
           if err != nil {
               return nil, fmt.Errorf("batch %d-%d: %w", i, end, err)
           }
           allEmbeddings = append(allEmbeddings, embeddings...)
       }
       return allEmbeddings, nil
   }
   ```

4. embedBatchWithRetry helper:
   - Makes single API call for batch
   - Uses cenkalti/backoff/v4 (already in go.mod) for retry
   - Retry config: 500ms initial, 10s max interval, 30s max elapsed
   - Checks for rate limit (HTTP 429) - retryable
   - Other errors are permanent (no retry)

   ```go
   func (e *Embedder) embedBatchWithRetry(ctx context.Context, texts []string) ([][]float32, error) {
       var embeddings [][]float32

       operation := func() error {
           resp, err := e.client.Embeddings.New(ctx, openai.EmbeddingNewParams{
               Input: openai.F(texts),
               Model: openai.F(openai.EmbeddingModelTextEmbedding3Small),
           })
           if err != nil {
               if isRateLimitError(err) {
                   return err  // Retryable
               }
               return backoff.Permanent(err)  // Not retryable
           }

           // Convert float64 to float32 for storage compatibility
           embeddings = make([][]float32, len(resp.Data))
           for i, data := range resp.Data {
               embeddings[i] = toFloat32(data.Embedding)
           }
           return nil
       }

       b := backoff.NewExponentialBackOff()
       b.InitialInterval = 500 * time.Millisecond
       b.MaxInterval = 10 * time.Second
       b.MaxElapsedTime = 30 * time.Second

       err := backoff.Retry(operation, backoff.WithContext(b, ctx))
       return embeddings, err
   }
   ```

5. Helper functions:
   - isRateLimitError: Check for HTTP 429 status
   - toFloat32: Convert []float64 to []float32 (OpenAI returns float64, storage uses float32)

Constants:
- EmbeddingModel = openai.EmbeddingModelTextEmbedding3Small
- EmbeddingDimension = 1536 (matches storage.VectorDimension)
- DefaultBatchSize = 500
  </action>
  <verify>
go build ./internal/embedding/...
  </verify>
  <done>
Embedder batches requests and retries with exponential backoff; compiles without errors
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `go build ./...` succeeds
2. internal/embedding/ package exports: Client, Embedder, NewClient, NewEmbedder, GenerateEmbeddings
3. go.mod includes github.com/openai/openai-go dependency
4. Embedder produces [][]float32 compatible with storage.Chunk.Embedding
</verification>

<success_criteria>
- OpenAI client validates API key on creation
- Embedder batches texts in groups of 500
- Embedder retries with exponential backoff on rate limits
- Returns [][]float32 matching storage model requirements
- All code compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-processing/02-03-SUMMARY.md`
</output>
